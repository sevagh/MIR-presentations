\documentclass[letter,11pt]{report}
%\setlength{\parindent}{0pt}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{minted}
\usepackage{subfig}
\usepackage{titling}
\usepackage{caption}
\setlength{\droptitle}{1cm}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[backend=biber,authordate,annotation]{biblatex-chicago}
\addbibresource{citations.bib}
\usepackage{titlesec}
 
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\begin{document}

\noindent\LARGE{\textbf{Pitch Tracking}}\\
\Large{MUMT 621 Presentation 4. March 16, 2021. Sevag Hanssian, 260398537}\\
%\vspace{0.15em}

\noindent\Large{\textbf{Summary}}
%\vspace{0.15em}

Pitch is the perceptual correlate of frequency (\cite{plack}). According to Plack, the American National Standards Institute's definition of pitch as ``that attribute of auditory sensation in terms of which sounds may be ordered on a scale extending from low to high'' is too ambiguous. The terms ``high'' and ``low'' can also apply to loudness and intensity. He proposes an alternative definition, that pitch is the aspect of auditory sensation whose variation is associated with musical melodies. For a pure tone consisting of a single frequency, the pitch of the tone is directly related to its frequency. For a complex tone with multiple frequency components, the pitch is related to the fundamental, or lowest frequency. The fundamental frequency is also referred to as $\mathit{f0}$.

Pitch is essential to music perception in humans (\cite{musicevo}). Relationships between pitches are more important than the absolute pitch -- humans in experiments can recognize melodies when they are transposed in tempo or octave. Pitches separated by the special octave relationship have the same pitch chroma. Most known music in the world across cultures and eras consists of melodies formed from five to seven pitches within an octave range.

\textcite{moore} states that for years there were two theories on human pitch perception: place and temporal. The place theory, or place coding, is related to the tonotopic organization of the inner ear: spectral analysis is done in the cochlea, and the resolved harmonics of a sound excite different parts of the basilar membrane (BM), causing with different characteristic frequency (CF) to fire. The activated center frequencies may then be compared to a pattern of harmonics (but this second postulate is under debate). In the temporal theory, the unresolved harmonics form a complex waveform in the BM, and firing neurons lock to the phase of the envelope of the complex waveform. More complete, newer models that try to account for all the available experimental data rely on both place and temporal analyses.

In computational pitch tracking, fundamental frequency ($\mathit{f0}$) and pitch are used interchangeably.

\vfill
\clearpage

Computational pitch tracking has been studied for at least half a century (\cite{nollcepstrum}). A common family of approaches involves selecting candidates from a generating function, and performing pre- and post-processing to produce the pitch curve -- among the candidate functions are cepstrum (\cite{nollcepstrum}), autocorrelation function (\cite{acorr}), average magnitude difference function (\cite{amdf}), normalized cross-correlation function (RAPT and PRAAT; \cite{rapt, praat}), cumulative mean normalized difference function (YIN; \cite{yin}), and the normalized square difference function (\cite{mpm}). Some forms of post-processing of the pitch candidate functions involves peak picking or parabolic interpolation.

Two more recent approaches include SWIPE (\cite{swipe}) which performs spectral template matching of the input sound against sawtooth waveforms, and pYIN (\cite{pyin}), a probablistic variant of YIN with a Hidden Markov Model (HMM) as a post-processing step to predict probable pitch sequences. The pitch space of pYIN is divided into 480 bins ranging over four octaves from A1 (55Hz) to A5 (880Hz) in steps of 10 cents. Finally, CREPE (\cite{crepe}) is the current state-of-the-art neural network for pitch tracking, and operates on the time-domain waveform directly, outputting a probability for 360 possible pitches ranging over six octaves between C1 (32.70Hz) and B7 (1975.5Hz) in steps of 20 cents. CREPE describes previous approaches as DSP-pipeline and heuristic-based, while claiming to be the first data-driven pitch tracker. The results of CREPE show it beating pYIN, the previous state-of-the-art according to several survey papers (\cite{comparison1, comparison2}).

Another way to categorize pitch tracking algorithms is by the domain (time or frequency) in which they operate. \textcite{comparison3} provide yet another comprehensive survey of pitch tracking algorithms. Time domain methods include the previously mentioned autocorrelation function (ACF), YIN and RAPT, in addition to time domain excitation extraction based on a minimum perturbation operator (TEMPO; \cite{tempo1, tempo2}). Frequency domain methods include the previously mentioned SWIPE. Finally, there are a set of algorithms combine both approaches such as Aurora (\cite{aurora}), NDF (\cite{ndf}), and REAPER.\footnote{\href{https://github.com/google/REAPER}{https://github.com/google/REAPER}}

\vfill
\clearpage

\noindent\LARGE{\textbf{Bibliography}}\\

\vspace{-0.5em}

%\printbibheading[title={\vspace{-3.5em}References},heading=bibnumbered]
%\nocite{*}
\printbibliography[heading=none]

\end{document}
