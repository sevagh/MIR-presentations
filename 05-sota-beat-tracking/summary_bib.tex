\documentclass[letter,11pt]{report}
%\setlength{\parindent}{0pt}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{minted}
\usepackage{subfig}
\usepackage{titling}
\usepackage{caption}
\setlength{\droptitle}{1cm}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[backend=biber,authordate,annotation]{biblatex-chicago}
\addbibresource{citations.bib}
\usepackage{titlesec}

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{enumerate}}

\newenvironment{tight_itemize}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{itemize}}
 
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\begin{document}

\noindent\LARGE{\textbf{State-of-the-art Beat Tracking}}\\
\Large{MUMT 621 Presentation 5. March 30, 2021. Sevag Hanssian, 260398537}\\
%\vspace{0.15em}

\noindent\Large{\textbf{Summary}}

\textcite{mcfee} state that onset detection is a critical first stage of most beat tracking algorithms. Works by \textcite{goto}, \textcite{ellis}, and \textcite{btrack} are some examples among many of onset-based beat tracking. \textcite{onsets} provide multiple definitions for onsets, but the simplest is that of the beginning of musical events. A typical onset-based beat tracker, as described by \textcite{mcfee}, operates as follows:
\begin{quote}
	\vspace{-0.25em}
	First, the audio signal is processed by an onset strength function, which measures the likelihood that a musically salient change (e.g., note onset) has occurred at each time point. The tracking algorithm then selects the beat times from among the peaks of the onset strength profile
\end{quote}

\textcite{rnnonset} used a recurrent neural network architecture to achieve state-of-the-art results in onset detection. This was first adapted by \textcite{bock1} to create the ``RNNBeatProcessor,'' so named in the open-source Python madmom library (\cite{madmom}). The network architecture is based on Bi-directional Long Short-Term Memory recurrent neural networks. This network architecture was chosen for the following reasons:

\begin{tight_itemize}
	\vspace{-0.25em}
	\item
		In the most basic form of feed-forward neural network, the relationship of inputs to outputs is strictly causal, i.e., inputs at the current time compute outputs at the current time.
	\item
		To introduce using inputs from the past to influence current outputs, which is important in beat tracking, cycles are created in the network, leading to recurrent neural networks. However, these suffer from the vanishing gradient problem, which causes inputs to decay or blow up exponentially over time.
	\item
		The Long Short-Term Memory (LSTM) network solves the problem of the vanishing gradient by introducing memory gates within the recurrent unit.
	\item
		Finally, to consider inputs from the future in the output, a Bi-directional LSTM (BLSTM) network introduces a second hidden layer which introduces the inputs to the network in a reverse temporal order.
\end{tight_itemize}

As past, present, and future inputs are all useful in a beat tracking algorithm, the BLSTM network from \textcite{rnnonset} was considered to be an appropriate choice. An additional modification was to introduce a peak picking stage for beat selection.

In their next paper, \textcite{bock3} adapted the RNNBeatProcessor to introduce multiple RNN models, each of which was trained on a different musical style, and added a dynamic Bayesian network in the front-end for the beat estimation. The choice of dynamic Bayesian network was based on prior work by \textcite{whiteley}, which achieved robust results in joint tempo and beat estimation.

In yet another follow-up paper, \textcite{bock2} modified the probabilistic tempo and beat sequence model of \cite{whiteley} to make it more computationally efficient (both in CPU cycles and memory) while maintaining the high accuracy of their solution in \cite{bock3}. The original joint tempo and beat model of \textcite{whiteley} is referred to as the \textit{tempo-bar model}, and it has two main deficiencies which were adjusted:

\begin{tight_enumerate}
	\vspace{-0.25em}
	\item
		The tempo-bar model assumes that human tempo discrimination is consistent across all tempi and use linearly-spaced tempi, while \textcite{bock2} describe that in fact humans have finer tempo resolution at lower tempos. To achieve a tempo spacing consistent with the just-noticeable-difference (JND) limits of human tempo resolution with the linearly-spaced model requires a huge number of tempi, while using a nonlinear spacing of more tempi in the low tempo range and fewer tempi in the high tempo range significantly reduces the total number of possible tempi and matches human perception better.
	\item
		The tempo-bar model allows tempo transitions at any possible time, while \textcite{bock2} limit their more efficient model to only allow tempo transitions on beat locations, which is consistent with tempo transitions in music.
\end{tight_enumerate}

The final algorithm for state-of-the-art beat tracking is therefore presented by \cite{bock2}, incorporating the algorithms from \cite{bock1} and \cite{bock3}, and achieving the best results in the last \textbf{M}usic \textbf{I}nformation \textbf{R}etrieval \textbf{E}valuation e\textbf{X}change audio beat tracking challenge.\footnote{\url{https://www.music-ir.org/mirex/wiki/2019:MIREX2019_Results}}

\vfill
\clearpage

\noindent\LARGE{\textbf{Bibliography}}\\

\vspace{-1em}

%\printbibheading[title={\vspace{-3.5em}References},heading=bibnumbered]
\nocite{*}
\printbibliography[heading=none]

\end{document}
